name: Deploy GCP Real-time Data Pipeline

on:
  push:
    branches: [main, dev]
  pull_request:
    branches: [main]
  workflow_dispatch:

env:
  PROJECT_ID: fabled-web-172810
  REGION: us-central1
  ENVIRONMENT: ${{ github.ref == 'refs/heads/main' && 'prod' || 'dev' }}
  GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}

jobs:
  deploy-pipeline:
    name: Clean Deploy Complete Pipeline
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/dev'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.5.7

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ env.GCP_SA_KEY }}

    - name: Setup Google Cloud SDK
      uses: google-github-actions/setup-gcloud@v2

    - name: Configure Docker for GCR
      run: gcloud auth configure-docker

    # ==============================
    # PHASE 1: COMPLETE CLEANUP
    # ==============================
    - name: 🧹 Complete Environment Cleanup
      run: |
        echo "🧹 Starting complete environment cleanup..."
        chmod +x ./scripts/cleanup-environment.sh
        ./scripts/cleanup-environment.sh ${{ env.PROJECT_ID }} ${{ env.ENVIRONMENT }} ${{ env.REGION }}

    # ==============================
    # PHASE 2: FRESH INFRASTRUCTURE
    # ==============================
    - name: 🏗️ Initialize Terraform (Fresh)
      working-directory: ./terraform
      run: |
        echo "🏗️ Initializing Terraform with fresh state..."
        terraform init

    - name: 🔍 Validate Terraform Configuration
      working-directory: ./terraform
      run: terraform validate

    - name: 📋 Create Terraform Plan
      working-directory: ./terraform
      run: |
        echo "📋 Creating fresh deployment plan..."
        terraform plan -out=tfplan
      env:
        TF_VAR_project_id: ${{ env.PROJECT_ID }}
        TF_VAR_environment: ${{ env.ENVIRONMENT }}

    - name: 🔄 Deploy Dataflow Template
      run: |
        echo "🔄 Building and deploying Dataflow template..."
        chmod +x ./scripts/deploy-dataflow-template.sh
        ./scripts/deploy-dataflow-template.sh ${{ env.PROJECT_ID }} ${{ env.ENVIRONMENT }} ${{ env.REGION }}

    - name: 🚀 Apply Infrastructure
      working-directory: ./terraform
      run: |
        echo "🚀 Deploying infrastructure..."
        terraform apply -auto-approve tfplan

    - name: 🔐 Assign IAM Roles
      run: |
        echo "🔐 Assigning IAM roles to service accounts..."
        chmod +x ./scripts/assign-iam-roles.sh
        ./scripts/assign-iam-roles.sh ${{ env.PROJECT_ID }} ${{ env.ENVIRONMENT }}
        
        echo "⏳ Waiting for IAM propagation..."
        sleep 30

    # ==============================
    # PHASE 3: APPLICATION DEPLOYMENT
    # ==============================
    - name: ⚡ Deploy Cloud Function
      run: |
        echo "⚡ Deploying BigQuery Table Manager Cloud Function..."
        chmod +x ./scripts/deploy-cloud-function.sh
        ./scripts/deploy-cloud-function.sh ${{ env.PROJECT_ID }} ${{ env.ENVIRONMENT }}

    - name: 🏃 Deploy Event Generator
      run: |
        echo "🏃 Deploying Event Generator Cloud Run service..."
        chmod +x ./scripts/deploy-event-generator.sh
        ./scripts/deploy-event-generator.sh ${{ env.PROJECT_ID }} ${{ env.ENVIRONMENT }} ${{ env.REGION }}

    # ==============================
    # PHASE 4: VERIFICATION
    # ==============================
    - name: 🧪 Integration Tests
      run: |
        echo "🧪 Running integration tests..."
        chmod +x ./scripts/test-table-creation.sh
        ./scripts/test-table-creation.sh ${{ env.PROJECT_ID }} ${{ env.ENVIRONMENT }}

    - name: ✅ Verify Deployment
      run: |
        echo "✅ Verifying complete deployment..."
        
        # Check BigQuery Dataset
        DATASET="${{ env.ENVIRONMENT }}_events_dataset"
        echo "📊 Checking BigQuery dataset: $DATASET"
        bq ls --project_id=${{ env.PROJECT_ID }} $DATASET
        
        # Check Dataflow Job
        echo "🔄 Checking Dataflow jobs..."
        gcloud dataflow jobs list --region=${{ env.REGION }} --filter="name:${{ env.ENVIRONMENT }}-*" --limit=5
        
        # Check Table Manager Cloud Run Service
        echo "⚡ Checking Table Manager Cloud Run service..."
        gcloud run services describe ${{ env.ENVIRONMENT }}-table-manager --region=${{ env.REGION }} --format="value(status.conditions[0].status)"
        
        # Check Event Generator Cloud Run Service  
        echo "🏃 Checking Event Generator Cloud Run service..."
        gcloud run services describe ${{ env.ENVIRONMENT }}-event-generator --region=${{ env.REGION }} --format="value(status.conditions[0].status)"
        
        echo "🎉 All services verified successfully!"

    # ==============================
    # PHASE 5: START TEST LOAD
    # ==============================
    - name: 🧪 Start Medium Test Load
      run: |
        echo "🧪 Starting medium test load to verify complete pipeline..."
        
        # Get Event Generator service URL
        EVENT_GENERATOR_URL=$(gcloud run services describe ${{ env.ENVIRONMENT }}-event-generator --region=${{ env.REGION }} --format="value(status.url)")
        echo "📍 Event Generator URL: $EVENT_GENERATOR_URL"
        
        # Wait for services to be fully ready
        sleep 30
        
        # Start a medium test load scenario
        echo "🚀 Starting medium test load (5 minutes, 60 events/minute)..."
        curl -X POST "$EVENT_GENERATOR_URL/generate/start" \
          -H "Content-Type: application/json" \
          -d '{
            "events_per_minute": 60,
            "duration_minutes": 5,
            "event_types": ["order", "inventory", "user_activity"]
          }' \
          --fail --show-error --silent | jq .
        
        echo "✅ Medium test load started successfully!"
        echo "📊 This will generate 300 events over 5 minutes to test the pipeline"
        echo "🔍 Events will flow: Event Generator → Pub/Sub → Table Manager → BigQuery"

    - name: 🔍 Verify Event Flow and Table Creation
      run: |
        echo "🔍 Testing complete event pipeline flow..."
        
        EVENT_GENERATOR_URL=$(gcloud run services describe ${{ env.ENVIRONMENT }}-event-generator --region=${{ env.REGION }} --format="value(status.url)")
        
        # Wait for services to initialize
        sleep 10
        
        # Test 1: Generate a few individual events to trigger table creation
        echo "🧪 Test 1: Generating individual events to trigger table creation..."
        for event_type in order inventory user_activity; do
          echo "📤 Publishing $event_type event..."
          curl -X POST "$EVENT_GENERATOR_URL/generate/single/$event_type" \
            -H "Content-Type: application/json" \
            --fail --silent || echo "⚠️  Failed to publish $event_type event"
          sleep 2
        done
        
        # Wait for table creation
        echo "⏳ Waiting 60 seconds for table creation..."
        sleep 60
        
        # Test 2: Check if BigQuery tables were created
        echo "🔍 Test 2: Checking BigQuery table creation..."
        DATASET="${{ env.ENVIRONMENT }}_events_dataset"
        
        for table in orders inventory user_activity; do
          echo "📊 Checking table: $table"
          if bq show --project_id=${{ env.PROJECT_ID }} $DATASET.$table 2>/dev/null; then
            echo "✅ Table $table exists!"
          else
            echo "❌ Table $table not found"
          fi
        done
        
        # Test 3: Check Pub/Sub topic activity
        echo "🔍 Test 3: Checking Pub/Sub topic activity..."
        TOPIC_NAME="backend-events-topic"
        echo "📨 Checking topic: $TOPIC_NAME"
        gcloud pubsub topics describe $TOPIC_NAME --format="value(name)" || echo "❌ Topic not found"
        
        # Test 4: Check subscription activity  
        echo "🔍 Test 4: Checking subscription activity..."
        SUB_NAME="backend-events-topic-sub"
        echo "📨 Checking subscription: $SUB_NAME"
        gcloud pubsub subscriptions describe $SUB_NAME --format="value(name)" || echo "❌ Subscription not found"
        
        echo "✅ Event flow verification completed!"

    - name: 📊 Monitor Pipeline Health
      run: |
        echo "📊 Monitoring pipeline health for 2 minutes..."
        
        EVENT_GENERATOR_URL=$(gcloud run services describe ${{ env.ENVIRONMENT }}-event-generator --region=${{ env.REGION }} --format="value(status.url)")
        
        # Check pipeline components every 30 seconds for 2 minutes
        for i in {1..4}; do
          echo "🔍 Health check $i/4..."
          
          # Check Event Generator health
          curl -f "$EVENT_GENERATOR_URL/health" --silent | jq .
          
          # Check Table Manager health  
          TABLE_MANAGER_URL=$(gcloud run services describe ${{ env.ENVIRONMENT }}-table-manager --region=${{ env.REGION }} --format="value(status.url)")
          curl -f "$TABLE_MANAGER_URL/health" --silent | jq .
          
          # Check if tables are being created
          DATASET="${{ env.ENVIRONMENT }}_events_dataset"
          TABLES=$(bq ls --project_id=${{ env.PROJECT_ID }} $DATASET --format="value(tableId)" 2>/dev/null | wc -l)
          echo "📊 BigQuery tables created: $TABLES"
          
          sleep 30
        done
        
        echo "✅ Pipeline monitoring completed!"

  # ==============================
  # CLEANUP ON FAILURE
  # ==============================
  cleanup-on-failure:
    name: Cleanup on Failure
    runs-on: ubuntu-latest
    needs: deploy-pipeline
    if: failure() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/dev')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ env.GCP_SA_KEY }}

    - name: Setup Google Cloud SDK
      uses: google-github-actions/setup-gcloud@v2

    - name: 🧹 Emergency Cleanup
      run: |
        echo "🚨 Deployment failed - running emergency cleanup..."
        chmod +x ./scripts/cleanup-environment.sh
        ./scripts/cleanup-environment.sh ${{ env.PROJECT_ID }} ${{ env.ENVIRONMENT }} ${{ env.REGION }}
        echo "✅ Emergency cleanup completed" 