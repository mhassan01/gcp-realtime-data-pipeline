name: Deploy Real-time Data Pipeline Infrastructure

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}
  REGION: us-central1
  ENVIRONMENT: dev

jobs:
  terraform-plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.5.7

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ env.GCP_SA_KEY }}

    - name: Setup Google Cloud SDK
      uses: google-github-actions/setup-gcloud@v2

    - name: Terraform Init
      working-directory: ./terraform
      run: terraform init

    - name: Terraform Validate
      working-directory: ./terraform
      run: terraform validate

    - name: Terraform Plan
      working-directory: ./terraform
      run: terraform plan -out=tfplan
      env:
        TF_VAR_project_id: ${{ env.PROJECT_ID }}
        TF_VAR_environment: ${{ env.ENVIRONMENT }}

    - name: Upload Terraform Plan
      uses: actions/upload-artifact@v4
      with:
        name: terraform-plan
        path: terraform/tfplan

  deploy-infrastructure:
    name: Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: terraform-plan
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/dev'
    
    outputs:
      terraform_success: ${{ steps.terraform.outputs.success }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.5.7

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ env.GCP_SA_KEY }}

    - name: Setup Google Cloud SDK
      uses: google-github-actions/setup-gcloud@v2

    - name: Download Terraform Plan
      uses: actions/download-artifact@v4
      with:
        name: terraform-plan
        path: terraform/

    - name: Terraform Init
      working-directory: ./terraform
      run: terraform init

    - name: Terraform Apply
      id: terraform
      working-directory: ./terraform
      run: |
        terraform apply -auto-approve tfplan
        echo "success=true" >> $GITHUB_OUTPUT
      env:
        TF_VAR_project_id: ${{ env.PROJECT_ID }}
        TF_VAR_environment: ${{ env.ENVIRONMENT }}

    - name: Handle Resource Conflicts (if needed)
      if: failure()
      working-directory: ./terraform
      run: |
        echo "üîç Handling potential resource conflicts..."
        
        ENV="${{ env.ENVIRONMENT }}"
        PROJECT_ID="${{ env.PROJECT_ID }}"
        
        # Import existing resources to resolve conflicts
        echo "Attempting to import conflicting resources..."
        terraform import -var="project_id=$PROJECT_ID" -var="environment=$ENV" google_pubsub_topic.backend_events "projects/$PROJECT_ID/topics/$ENV-backend-events-topic" || true
        terraform import -var="project_id=$PROJECT_ID" -var="environment=$ENV" google_pubsub_topic.dead_letter "projects/$PROJECT_ID/topics/$ENV-backend-events-dead-letter" || true
        terraform import -var="project_id=$PROJECT_ID" -var="environment=$ENV" google_bigquery_dataset.events_dataset "projects/$PROJECT_ID/datasets/${ENV}_events_dataset" || true
        
        # Try applying again
        echo "Retrying terraform apply after imports..."
        terraform apply -auto-approve
        echo "success=true" >> $GITHUB_OUTPUT
      env:
        TF_VAR_project_id: ${{ env.PROJECT_ID }}
        TF_VAR_environment: ${{ env.ENVIRONMENT }}

    - name: Assign IAM Roles
      if: steps.terraform.outputs.success == 'true' || success()
      run: |
        echo "üîê Assigning IAM roles to service accounts..."
        chmod +x ./scripts/assign-iam-roles.sh
        ./scripts/assign-iam-roles.sh ${{ env.PROJECT_ID }} ${{ env.ENVIRONMENT }}
        
        echo "‚è≥ Waiting for IAM propagation..."
        sleep 30

  deploy-applications:
    name: Deploy Applications
    runs-on: ubuntu-latest
    needs: deploy-infrastructure
    if: needs.deploy-infrastructure.outputs.terraform_success == 'true'
    
    strategy:
      matrix:
        service: [cloud-function, event-generator, dataflow-template]
        
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ env.GCP_SA_KEY }}

    - name: Setup Google Cloud SDK
      uses: google-github-actions/setup-gcloud@v2

    - name: Configure Docker for GCR
      if: matrix.service == 'event-generator' || matrix.service == 'dataflow-template'
      run: gcloud auth configure-docker

    - name: Deploy Cloud Function
      if: matrix.service == 'cloud-function'
      run: |
        echo "üöÄ Deploying Cloud Function for table management..."
        chmod +x ./scripts/deploy-cloud-function.sh
        ./scripts/deploy-cloud-function.sh ${{ env.PROJECT_ID }} ${{ env.ENVIRONMENT }}

    - name: Deploy Event Generator
      if: matrix.service == 'event-generator'
      run: |
        echo "üöÄ Deploying Event Generator to Cloud Run..."
        chmod +x ./scripts/deploy-event-generator.sh
        ./scripts/deploy-event-generator.sh ${{ env.PROJECT_ID }} ${{ env.ENVIRONMENT }} ${{ env.REGION }}

    - name: Deploy Dataflow Template
      if: matrix.service == 'dataflow-template'
      run: |
        echo "üöÄ Building and deploying Dataflow template..."
        chmod +x ./scripts/deploy-dataflow-template.sh
        ./scripts/deploy-dataflow-template.sh ${{ env.PROJECT_ID }} ${{ env.ENVIRONMENT }} ${{ env.REGION }}

  deploy-dataflow-job:
    name: Deploy Dataflow Job
    runs-on: ubuntu-latest
    needs: deploy-applications
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ env.GCP_SA_KEY }}

    - name: Setup Google Cloud SDK
      uses: google-github-actions/setup-gcloud@v2

    - name: Deploy Dataflow Streaming Job
      run: |
        echo "üöÄ Starting Dataflow streaming pipeline..."
        
        PROJECT_ID="${{ env.PROJECT_ID }}"
        ENV="${{ env.ENVIRONMENT }}"
        REGION="${{ env.REGION }}"
        
        # Check if template exists
        TEMPLATE_PATH="gs://$PROJECT_ID-$ENV-dataflow-templates/templates/streaming-pipeline.json"
        if ! gsutil ls "$TEMPLATE_PATH" > /dev/null 2>&1; then
          echo "‚ùå Template not found at $TEMPLATE_PATH"
          exit 1
        fi
        
        # Check if job already exists
        EXISTING_JOBS=$(gcloud dataflow jobs list --region=$REGION --filter="name:$ENV-realtime-data-pipeline AND state:Running" --format="value(id)")
        if [ ! -z "$EXISTING_JOBS" ]; then
          echo "‚ö†Ô∏è Dataflow job already running: $EXISTING_JOBS"
          echo "Skipping job creation"
          exit 0
        fi
        
        # Create Dataflow job
        echo "Creating Dataflow job from template..."
        gcloud dataflow jobs run "$ENV-realtime-data-pipeline" \
          --gcs-location="$TEMPLATE_PATH" \
          --region="$REGION" \
          --service-account-email="$ENV-dataflow-pipeline-sa@$PROJECT_ID.iam.gserviceaccount.com" \
          --parameters="inputSubscription=projects/$PROJECT_ID/subscriptions/$ENV-backend-events-subscription,outputDataset=${ENV}_events_dataset,outputGcsPrefix=gs://$PROJECT_ID-$ENV-raw-events/output,project=$PROJECT_ID,region=$REGION,environment=$ENV" \
          --max-workers=10 \
          --num-workers=2 \
          --worker-machine-type=n1-standard-2 \
          --network=default \
          --subnetwork=regions/$REGION/subnetworks/default \
          --enable-streaming-engine
        
        echo "‚úÖ Dataflow job started successfully"

  test-pipeline:
    name: Test Complete Pipeline
    runs-on: ubuntu-latest
    needs: [deploy-applications, deploy-dataflow-job]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ env.GCP_SA_KEY }}

    - name: Setup Google Cloud SDK
      uses: google-github-actions/setup-gcloud@v2

    - name: Wait for Services to be Ready
      run: |
        echo "‚è≥ Waiting for all services to be fully ready..."
        sleep 90

    - name: Test Event Generator Service
      run: |
        echo "üß™ Testing Event Generator service..."
        
        SERVICE_NAME="${{ env.ENVIRONMENT }}-event-generator"
        SERVICE_URL=$(gcloud run services describe $SERVICE_NAME --region=${{ env.REGION }} --format="value(status.url)")
        
        if [ -z "$SERVICE_URL" ]; then
          echo "‚ùå Could not get Event Generator service URL"
          exit 1
        fi
        
        echo "Testing Event Generator at: $SERVICE_URL"
        
        # Test health endpoint
        curl -f "$SERVICE_URL/health" || exit 1
        echo "‚úÖ Health check passed"
        
        # Test sample event generation
        curl -f "$SERVICE_URL/sample/order" || exit 1
        echo "‚úÖ Sample event generation working"
        
        # Test scenarios endpoint
        curl -f "$SERVICE_URL/scenarios" || exit 1
        echo "‚úÖ Scenarios endpoint working"

    - name: Test Cloud Function
      run: |
        echo "üß™ Testing Cloud Function..."
        
        FUNCTION_NAME="${{ env.ENVIRONMENT }}-bigquery-table-manager"
        
        # Check if function exists
        if gcloud functions describe $FUNCTION_NAME --region=${{ env.REGION }} > /dev/null 2>&1; then
          echo "‚úÖ Cloud Function deployed successfully"
        else
          echo "‚ùå Cloud Function not found"
          exit 1
        fi

    - name: Verify Dataflow Job
      run: |
        echo "üß™ Verifying Dataflow job..."
        
        JOB_NAME="${{ env.ENVIRONMENT }}-realtime-data-pipeline"
        JOB_ID=$(gcloud dataflow jobs list --region=${{ env.REGION }} --filter="name:${JOB_NAME}" --format="value(id)" | head -1)
        
        if [ -z "$JOB_ID" ]; then
          echo "‚ùå Dataflow job not found"
          exit 1
        fi
        
        JOB_STATE=$(gcloud dataflow jobs describe $JOB_ID --region=${{ env.REGION }} --format="value(currentState)")
        echo "Dataflow job state: $JOB_STATE"
        
        if [ "$JOB_STATE" = "JOB_STATE_RUNNING" ]; then
          echo "‚úÖ Dataflow job is running"
        else
          echo "‚ö†Ô∏è Dataflow job state: $JOB_STATE"
        fi

    - name: Test BigQuery Setup
      run: |
        echo "üß™ Testing BigQuery setup..."
        
        DATASET="${{ env.ENVIRONMENT }}_events_dataset"
        
        # Check if dataset exists
        if bq show --project_id=${{ env.PROJECT_ID }} $DATASET > /dev/null 2>&1; then
          echo "‚úÖ BigQuery dataset exists"
          
          # List tables in dataset
          echo "Tables in dataset:"
          bq ls --project_id=${{ env.PROJECT_ID }} $DATASET
        else
          echo "‚ùå BigQuery dataset not found"
          exit 1
        fi

    - name: Run Integration Test
      run: |
        echo "üß™ Running end-to-end integration test..."
        
        if [ -f "./scripts/test-table-creation.sh" ]; then
          chmod +x ./scripts/test-table-creation.sh
          ./scripts/test-table-creation.sh ${{ env.PROJECT_ID }} ${{ env.ENVIRONMENT }}
        else
          echo "‚ÑπÔ∏è Integration test script not found, skipping"
        fi

    - name: Generate Test Events
      run: |
        echo "üß™ Generating test events..."
        
        SERVICE_NAME="${{ env.ENVIRONMENT }}-event-generator"
        SERVICE_URL=$(gcloud run services describe $SERVICE_NAME --region=${{ env.REGION }} --format="value(status.url)")
        
        if [ ! -z "$SERVICE_URL" ]; then
          echo "Generating sample events via Event Generator..."
          curl -X POST "$SERVICE_URL/generate/batch" \
            -H "Content-Type: application/json" \
            -d '{"count": 10, "event_types": ["order", "inventory", "user_activity"]}' || true
          echo "‚úÖ Test events generated"
        fi

  deployment-summary:
    name: Deployment Summary
    runs-on: ubuntu-latest
    needs: [deploy-infrastructure, deploy-applications, deploy-dataflow-job, test-pipeline]
    if: always()
    
    steps:
    - name: Deployment Status Summary
      run: |
        echo "üéØ Real-time Data Pipeline Deployment Summary"
        echo "=============================================="
        echo ""
        echo "üìã Infrastructure Status:"
        echo "  - Terraform Apply: ${{ needs.deploy-infrastructure.result }}"
        echo "  - IAM Roles: Assigned via script"
        echo ""
        echo "üìã Application Status:"
        echo "  - Cloud Function: ${{ needs.deploy-applications.result }}"
        echo "  - Event Generator: ${{ needs.deploy-applications.result }}" 
        echo "  - Dataflow Template: ${{ needs.deploy-applications.result }}"
        echo ""
        echo "üìã Pipeline Status:"
        echo "  - Dataflow Job: ${{ needs.deploy-dataflow-job.result }}"
        echo "  - Integration Tests: ${{ needs.test-pipeline.result }}"
        echo ""
        echo "üåê Access Points:"
        echo "  - Event Generator: https://console.cloud.google.com/run"
        echo "  - BigQuery Console: https://console.cloud.google.com/bigquery"
        echo "  - Dataflow Console: https://console.cloud.google.com/dataflow"
        echo ""
        echo "üìä Next Steps:"
        echo "  1. Monitor Dataflow job in GCP Console"
        echo "  2. Generate events via Event Generator API"
        echo "  3. Query processed data in BigQuery"
        echo "  4. Check Cloud Function logs for table creation"
        echo ""
        if [ "${{ needs.deploy-infrastructure.result }}" = "success" ] && [ "${{ needs.deploy-applications.result }}" = "success" ]; then
          echo "üéâ Deployment completed successfully!"
        else
          echo "‚ö†Ô∏è Some components may have issues. Check job logs."
        fi

  cleanup-on-failure:
    name: Cleanup on Failure
    runs-on: ubuntu-latest
    needs: [deploy-infrastructure, deploy-applications, deploy-dataflow-job, test-pipeline]
    if: failure()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ env.GCP_SA_KEY }}

    - name: Setup Google Cloud SDK
      uses: google-github-actions/setup-gcloud@v2

    - name: Run Cleanup Script
      run: |
        if [ -f "./scripts/cleanup-all-resources.sh" ]; then
          chmod +x ./scripts/cleanup-all-resources.sh
          ./scripts/cleanup-all-resources.sh --auto-approve --project-id="${{ env.PROJECT_ID }}" --environment="${{ env.ENVIRONMENT }}"
        else
          echo "‚ö†Ô∏è Cleanup script not found, performing manual cleanup..."
          
          ENV="${{ env.ENVIRONMENT }}"
          PROJECT_ID="${{ env.PROJECT_ID }}"
          REGION="${{ env.REGION }}"
          
          # Stop Dataflow jobs
          gcloud dataflow jobs list --region=$REGION --filter="name:$ENV-realtime-data-pipeline" --format="value(id)" | while read job_id; do
            [ ! -z "$job_id" ] && gcloud dataflow jobs cancel $job_id --region=$REGION || true
          done
          
          # Delete Cloud Run and Cloud Function
          gcloud run services delete "$ENV-event-generator" --region=$REGION --quiet || true
          gcloud functions delete "$ENV-bigquery-table-manager" --region=$REGION --quiet || true
          
          echo "üßπ Basic cleanup completed"
        fi
      continue-on-error: true 